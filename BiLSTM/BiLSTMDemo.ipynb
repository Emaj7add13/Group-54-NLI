{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "73vPxqk0fqka"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D, Masking, Reshape, Input, concatenate, Flatten, BatchNormalization, Attention, Dot, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import time\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBgHc40ShBDF",
        "outputId": "5fc0d670-3e34-43c4-baec-de5c386edc87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rEoIAR1Dfqkj"
      },
      "outputs": [],
      "source": [
        "#Preprocessing\n",
        "def prep(file):\n",
        "  df=pd.read_csv(file)\n",
        "  df=df.fillna(value='')\n",
        "  premise=df['premise'].tolist()\n",
        "  hypothesis=df['hypothesis'].tolist()\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  p_pre=[]\n",
        "  h_pre=[]\n",
        "  for i in range(len(premise)):\n",
        "    p_token=[word for word in premise[i] if word not in string.punctuation]\n",
        "    p_token=[word.lower() for word in p_token]\n",
        "    lemma_p=[lemmatizer.lemmatize(word) for word in p_token]\n",
        "    p_pre.append(lemma_p)\n",
        "\n",
        "    h_token=[word for word in hypothesis[i] if word not in string.punctuation]\n",
        "    h_token=[word.lower() for word in h_token]\n",
        "    lemma_h=[lemmatizer.lemmatize(word) for word in h_token]\n",
        "    h_pre.append(lemma_h)\n",
        "\n",
        "  tokenizer = Tokenizer(num_words = 10000)\n",
        "  tokenizer.fit_on_texts(p_pre)\n",
        "  vocab_size = 10000\n",
        "\n",
        "  max_sequence_length=120\n",
        "  p_seq = tokenizer.texts_to_sequences(p_pre)\n",
        "  padded_p = pad_sequences(p_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "  h_seq = tokenizer.texts_to_sequences(h_pre)\n",
        "  padded_h = pad_sequences(h_seq, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "  return padded_p, padded_h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aaZfhQQofqkl"
      },
      "outputs": [],
      "source": [
        "model=load_model('BiLSTM.h5')\n",
        "test_p, test_h=prep('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZAO7Mm3fqkn",
        "outputId": "70000ba6-9dcf-43a9-d1ce-99687ceb73ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104/104 [==============================] - 9s 69ms/step\n"
          ]
        }
      ],
      "source": [
        "test_pred=model.predict([test_p, test_h])\n",
        "test_ps=np.argmax(test_pred, axis=1)\n",
        "df_test=pd.DataFrame({'prediction': test_ps})\n",
        "df_test.to_csv('Group_54_B.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}